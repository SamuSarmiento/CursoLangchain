{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "907ba62a",
   "metadata": {},
   "source": [
    "# Gu칤a de inicio r치pido"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a58bea8",
   "metadata": {},
   "source": [
    "Este tutorial le brindar치 una visi칩n general y concisa sobre c칩mo construir una aplicaci칩n de modelo de lenguaje de extremo a extremo utilizando LangChain.\n",
    "\n",
    ">Docs: https://python.langchain.com/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c2cb957",
   "metadata": {},
   "source": [
    "**LangChain** es una biblioteca Python, creada por **Harrison Chase**, de c칩digo abierto dise침ada espec칤ficamente para proporcionar a los desarrolladores las herramientas necesarias para crear aplicaciones basadas en grandes modelos ling칲칤sticos (LLM) de manera eficiente y profesional.\n",
    "\n",
    "Con LangChain, los desarrolladores pueden conectarse f치cilmente a diversas fuentes de datos y de c치lculo, permiti칠ndoles crear aplicaciones que realicen tareas de procesamiento del lenguaje natural (PLN) en fuentes de datos espec칤ficas de dominio, repositorios privados y mucho m치s.\n",
    "\n",
    "Es un marco para la creaci칩n de agentes capaces de razonar y encadenar tareas. Brindando a los desarrolladores la capacidad de crear agentes inteligentes capaces de razonar sobre problemas complejos y dividirlos en subtareas m치s peque침as. Con LangChain, es posible introducir contexto y memoria en las finalizaciones al crear pasos intermedios y encadenar comandos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba602cab",
   "metadata": {},
   "source": [
    "## Instalaci칩n\n",
    "Para empezar, instala LangChain con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ef425b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (0.0.187)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (2.0.15)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f13c4d57",
   "metadata": {},
   "source": [
    "## Configuraci칩n del entorno\n",
    "Usar LangChain normalmente requerir치 integraciones con uno o m치s proveedores de modelos, almacenes de datos, apis, etc.\n",
    "Para este ejemplo, vamos a utilizar las APIs de **OpenAI**, por lo que primero tendremos que instalar su SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283dec4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (0.27.7)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from requests>=2.20->openai) (2.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\langchain-tutotials\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40e78a71",
   "metadata": {},
   "source": [
    "A continuaci칩n, tendremos que establecer la variable de entorno en nuestro archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4190bc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8add50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d40bbf6",
   "metadata": {},
   "source": [
    "## Creaci칩n de una aplicaci칩n de modelos ling칲칤sticos\n",
    "Ahora que hemos instalado LangChain y configurado nuestro entorno, podemos empezar a construir nuestra aplicaci칩n de modelo de lenguaje.\n",
    "\n",
    "LangChain proporciona muchos m칩dulos que pueden ser utilizados para construir aplicaciones de modelos de lenguaje. Los m칩dulos pueden combinarse para crear aplicaciones m치s complejas, o utilizarse individualmente para aplicaciones sencillas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d8c664",
   "metadata": {},
   "source": [
    "### LLMs: Obtener predicciones de un modelo ling칲칤stico\n",
    "El bloque de construcci칩n m치s b치sico de LangChain es llamar a un LLM con alg칰n input. B치sicamente tener tu propio ChatGPT corriendo en tu terminal. \n",
    ">Consejo: Compara los precios del servicio ChatGPT Plus de OpenAI y sus precios para la API 游땦\n",
    "\n",
    "En esta lecci칩n aprenderemos a interactuar con ChatGPT para generar texto. Muchas aplicaciones que est치n valoradas en millones de d칩lares como JasperAI, CopyAI... hacen lo mismo como parte de su negocio principal.\n",
    "\n",
    "Veamos un ejemplo sencillo de c칩mo hacerlo. Para ello, primero tenemos que importar el LLM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94468c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dec84a0",
   "metadata": {},
   "source": [
    "Podemos entonces inicializar el wrapper con cualquier argumento. En este ejemplo, probablemente queremos que las salidas sean standard, as칤 que lo inicializaremos con una temperatura media/baja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7acfc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2eb1fa4",
   "metadata": {},
   "source": [
    "춰Ahora podemos llamarlo con alg칰n input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a99e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Autonomous Agents Solutions Inc.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes Autonomous Agents?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "030908e5",
   "metadata": {},
   "source": [
    "### Plantillas de Promps: Gesti칩n de prompts para LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b012f594",
   "metadata": {},
   "source": [
    "Llamar a un LLM es un gran primer paso, pero es s칩lo el principio. Normalmente, cuando utilizas un LLM en una aplicaci칩n, no env칤as el input del usuario directamente al LLM. En su lugar, es probable que tomes el input del usuario, construyas un prompt y luego lo env칤es al LLM.\n",
    "\n",
    "Por ejemplo, en el ejemplo anterior, el texto que pasamos estaba \"hardcoded\" para pedir el nombre de una empresa que fabrica agentes aut칩nomos. En este servicio, lo que querr칤amos hacer es tomar s칩lo la entrada del usuario describiendo lo que hace la empresa, y luego formatear el prompt con esa informaci칩n.\n",
    "\n",
    "Esto es f치cil de hacer con LangChain.\n",
    "\n",
    "Primero definamos la plantilla del prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5fe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de273d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes autonomous agent?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"autonomous agent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f38a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Autonomous Agents, Inc.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(product=\"autonomous agent\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79484db4",
   "metadata": {},
   "source": [
    "### Cadenas: Combina LLMs y prompts en flujos de trabajo de varios pasos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bb705f5",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos trabajado con los PromptTemplate y LLM de base por s칤 solos. Pero, por supuesto, una aplicaci칩n real no es s칩lo una primitiva, sino m치s bien una combinaci칩n de ellas.\n",
    "\n",
    "Una cadena en LangChain se compone de enlaces, que pueden ser primitivas como LLMs u otras cadenas.\n",
    "\n",
    "El tipo m치s b치sico de cadena es una LLMChain, que consiste en un PromptTemplate y un LLM.\n",
    "\n",
    "Extendiendo el ejemplo anterior, podemos construir una LLMChain que tome la entrada del usuario, la formatee con un PromptTemplate, y luego pase la respuesta formateada a un LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28f6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0172b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.3)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c00942b0",
   "metadata": {},
   "source": [
    "Ahora podemos crear una cadena muy simple que tomar치 la entrada del usuario, formatear치 el prompt con ella, y luego la enviar치 al LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95591b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ae5c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CloudSoft Solutions.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Software as a service\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dadcd871",
   "metadata": {},
   "source": [
    "Ya est치. Esta es la primera cadena: una LLM Chain. Este es uno de los tipos m치s simples de cadenas, pero entender c칩mo funciona te preparar치 para trabajar con cadenas m치s complejas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd9ab0f7",
   "metadata": {},
   "source": [
    "### Agentes: Cadenas de llamadas din치micas basadas en el input del usuario"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09285d61",
   "metadata": {},
   "source": [
    "Hasta ahora, las cadenas que hemos visto se ejecutaban en un orden predeterminado.\n",
    "\n",
    "Los agentes ya no lo hacen: utilizan un LLM para determinar qu칠 acciones realizar y en qu칠 orden. Una acci칩n puede consistir en utilizar una herramienta y observar su resultado, o en volver al usuario.\n",
    "\n",
    "Cuando se utilizan correctamente, los agentes pueden ser extremadamente potentes. En este tutorial, le mostramos c칩mo utilizar f치cilmente los agentes a trav칠s de la API m치s simple y de m치s alto nivel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "571fc83b",
   "metadata": {},
   "source": [
    "Para cargar agentes, debe comprender los siguientes conceptos:\n",
    "\n",
    "- **Herramienta**: Una funci칩n que realiza una tarea espec칤fica. Pueden ser cosas como: B칰squeda en Google, b칰squeda en bases de datos, Python REPL, otras cadenas. La interfaz para una herramienta es actualmente una funci칩n que se espera que tenga una cadena como entrada, con una cadena como salida.\n",
    "\n",
    "- **LLM**: El modelo de lenguaje que alimenta al agente.\n",
    "\n",
    "- **Agente**: El agente a utilizar. Debe ser una cadena que haga referencia a una clase de agente de soporte. Dado que este cuaderno se centra en la API m치s simple y de m치s alto nivel, s칩lo cubre el uso de los agentes est치ndar soportados. Si desea implementar un agente personalizado, consulte la documentaci칩n https://python.langchain.com/en/latest/getting_started/getting_started.html#agents-dynamically-call-chains-based-on-user-input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e3a12c4",
   "metadata": {},
   "source": [
    "Para este ejemplo, tambi칠n necesitar치s instalar el paquete SerpAPI Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa92f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://serpapi.com/\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e9a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6aead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar, vamos a cargar el modelo ling칲칤stico que vamos a utilizar para controlar el agente.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# A continuaci칩n, vamos a cargar algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, as칤 que tenemos que pasarlo.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Por 칰ltimo, vamos a inicializar un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 춰Ahora vamos a probarlo!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5353d05a",
   "metadata": {},
   "source": [
    "### Memoria: A침adir estado a cadenas y agentes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76827448",
   "metadata": {},
   "source": [
    "Hasta ahora, todas las cadenas y agentes por los que hemos pasado han sido sin estado. Pero a menudo, es posible que quieras que una cadena o agente tenga alg칰n concepto de \"memoria\" para que pueda recordar informaci칩n sobre sus interacciones anteriores. El ejemplo m치s claro y sencillo de esto es el dise침o de un chatbot: quieres que recuerde mensajes anteriores para que pueda utilizar su contexto para mantener una conversaci칩n mejor. Esto ser칤a un tipo de \"memoria a corto plazo\". En el lado m치s complejo, se podr칤a imaginar una cadena/agente que recordara piezas clave de informaci칩n a lo largo del tiempo: esto ser칤a una forma de \"memoria a largo plazo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba101095",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75620a38",
   "metadata": {},
   "source": [
    "## Creaci칩n de una aplicaci칩n de modelos ling칲칤sticos: Modelos de chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95342e69",
   "metadata": {},
   "source": [
    "Del mismo modo, puedes utilizar modelos de chat en lugar de LLM. Los modelos de chat son una variaci칩n de los modelos de lenguaje. Aunque los modelos de chat utilizan modelos ling칲칤sticos, la interfaz que exponen es un poco diferente: en lugar de exponer una API de \"entrada de texto, salida de texto\", exponen una interfaz en la que los \"mensajes de chat\" son las entradas y salidas.\n",
    "\n",
    "Las API de modelos de chat son bastante nuevas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dfa5f59",
   "metadata": {},
   "source": [
    "### Plantillas de mensajes de chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73fabc37",
   "metadata": {},
   "source": [
    "De forma similar a los LLMs, puedes hacer uso de plantillas utilizando un MessagePromptTemplate.\n",
    "\n",
    "Puedes construir un ChatPromptTemplate a partir de uno o m치s MessagePromptTemplates. Puede utilizar el format_prompt de ChatPromptTemplate - esto devuelve un PromptValue, que puede convertir en una cadena o en un objeto Message, dependiendo de si desea utilizar el valor formateado como entrada a un modelo llm o chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e75fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obt칠n una finalizaci칩n de chat a partir de los mensajes formateados\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b86b0410",
   "metadata": {},
   "source": [
    "### Cadenas con modelos de chat\n",
    "La LLMChain comentada en la secci칩n anterior tambi칠n puede utilizarse con modelos de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f73c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffde68eb",
   "metadata": {},
   "source": [
    "### Agentes con modelos de chat\n",
    "Los agentes tambi칠n se pueden utilizar con modelos de chat, puedes inicializar uno utilizando AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION como tipo de agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar, vamos a cargar el modelo de lenguaje que vamos a utilizar para controlar el agente.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# A continuaci칩n, vamos a cargar algunas herramientas a utilizar. Ten en cuenta que la herramienta `llm-math` utiliza un LLM, as칤 que tenemos que pasarlo.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Por 칰ltimo, vamos a inicializar un agente con las herramientas, el modelo de lenguaje y el tipo de agente que queremos utilizar.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a369a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 춰Ahora vamos a probar!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f6d1581",
   "metadata": {},
   "source": [
    "### Memoria: A침adir estado a cadenas y agentes\n",
    "Puedes utilizar la memoria con cadenas y agentes inicializados con modelos de chat. La principal diferencia entre esto y y l amemoria para LLMs es que en lugar de intentar condensar todos los mensajes anteriores en una cadena, podemos mantenerlos como su propio objeto de memoria 칰nico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7004166",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfee1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2a7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34357e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ead4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
